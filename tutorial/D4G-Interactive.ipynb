{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7278bb0a",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "D4G workshop\n",
    "\n",
    "Potsdam, 13.06.22\n",
    "\n",
    "Author: Caroline Arnold, DKRZ / Helmholtz AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4291e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to Introduction to Machine Learning! Today we are going to work through the typical lifecycle of a machine learning project. We will be working with data from the CyGNSS satellite mission to predict global ocean wind speed.\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "This tutorial could be done on a laptop, but for convenience we will use Google Colab.\n",
    "\n",
    "1. Go to https://colab.research.google.com\n",
    "2. Sign in with a google account\n",
    "3. Open a new notebook (File > New notebook)\n",
    "3. Clone the tutorial git repository by executing the following command (the \"!\" is important here!)\n",
    "\n",
    "```bash\n",
    "! git clone https://github.com/crlna16/d4g-tutorial\n",
    "```\n",
    "\n",
    "The data is stored separately in the subfolder `./data`\n",
    "\n",
    "### Today's Goals\n",
    "\n",
    "1. Walk through all stages of a machine learning project\n",
    "1. Train a neural network \n",
    "1. Learn strategies to improve your machine learning algorithm\n",
    "1. Optional: Get familiar with different neural network architectures\n",
    "\n",
    "This notebook can serve as a reference for you to employ in your own scientific machine learning projects. We do not expect you to understand every single line of code!\n",
    "\n",
    "\n",
    "<img src=\"./images/data-science-lifecycle8-3x.png\" alt=\"Data Science Lifecycle\" width=\"500\" height=\"500\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157e5ce",
   "metadata": {},
   "source": [
    "## <font color=\"#0000ff\" weight=\"bold\">Scoping</font>\n",
    "\n",
    "Before we start the machine learning project, it is important to gain an understanding of the problem at hand. \n",
    "\n",
    "The CyGNSS (Cyclone GNSS) is a system of microsatellites that measures GNSS reflected off the Earth's surface. We would like to predict the global ocean wind speed from CyGNSS measurements. This is a *regression problem* (the wind speed is a continuous variable). As opposed to a *classification problem* (e.g., distinguish pictures of cats and dogs).\n",
    "\n",
    "Data is available from http://podaac.jpl.nasa.gov. We have downloaded and processed a subset of CyGNSS data for the purpose of this tutorial. This dataset contains labels from ERA5 reanalysis data, i.e., we are dealing with *supervised learning*. Other, more advanced, machine learning methods are *unsupervised learning* and *reinforcement learning*.\n",
    "\n",
    "<img src=\"./images/cygnss-from-space.png\" alt=\"CyGNSS satellites and transmitter on top of a cyclone\" title=\"CyNGSS satellites, Image courtesy of Milad Asgarimehr\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd34df",
   "metadata": {},
   "source": [
    "## <font color=\"#0099cc\">Data cleaning</font>\n",
    "\n",
    "While it may be easy to obtain a lot of data, it is necessary to ensure the data quality, eg by checking for `None` values in the data. For the purpose of this tutorial, you can assume the data has been cleaned such that you can directly work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f9d09",
   "metadata": {},
   "source": [
    "## <font color=\"#33cc33\">Data exploration</font>\n",
    "\n",
    "Now it is time to take a look at the data. Check out the subdirectory `../data/`: There are three files, `train_data.nc`, `valid_data.nc`, `test_data.nc`. They can be opened with the Python library `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "np.random.seed(20220613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.open_dataset('../data/train_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc7b3d",
   "metadata": {},
   "source": [
    "Interactive browser for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11307934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28aa474",
   "metadata": {},
   "source": [
    "### Target variable\n",
    "\n",
    "The target variable is the wind speed. To extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ds_train['windspeed'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78332b",
   "metadata": {},
   "source": [
    "Visualization is very helpful for machine learning projects, as it helps us to identify the key properties of the dataset at a glance. We plot the distribution of the wind speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d665f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries for plotting. Check out https://seaborn.pydata.org/ for reference\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22688821",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y)\n",
    "\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.xticks([0, 2.5, 5.0, 7.5, 10, 12.5, 15, 17.5, 20])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19375701",
   "metadata": {},
   "source": [
    "### Feature variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3e61d",
   "metadata": {},
   "source": [
    "There are 2D and 1D variables in the dataset (use the interactive dataset browser to check that). First, we extract the 2D variables and look at some selected samples.\n",
    "\n",
    "#### BRCS (Bistatic Radar Cross Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "brcs = ds_train['brcs'][:]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "for i in range(5):\n",
    "    sns.heatmap(brcs[i*100], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2936f",
   "metadata": {},
   "source": [
    "\n",
    "#### 1D variables\n",
    "\n",
    "The dataset contains 1D variables as well. Here we can see the value ranges using histogram plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7778c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_nbrcs = ds_train['ddm_nbrcs'][:]\n",
    "ddm_les   = ds_train['ddm_les'][:]\n",
    "sp_inc_angle = ds_train['sp_inc_angle'][:]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "sns.histplot(ddm_nbrcs, ax=ax[0])\n",
    "ax[0].set_xlabel('DDM NBRCS')\n",
    "\n",
    "sns.histplot(ddm_les, ax=ax[1])\n",
    "ax[1].set_xlabel('DDM LES')\n",
    "\n",
    "sns.histplot(sp_inc_angle, ax=ax[2])\n",
    "ax[2].set_xlabel('SP INC ANGLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb51a9",
   "metadata": {},
   "source": [
    "### Relation of feature and target variables\n",
    "\n",
    "It is often insightful to look at density plots of feature and target variables. We do this here for the 1D variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, figsize=(20, 6))\n",
    "\n",
    "ax[0].hexbin(y, ddm_nbrcs, mincnt=1, cmap='viridis')\n",
    "ax[1].hexbin(y, ddm_les, mincnt=1, cmap='viridis')\n",
    "ax[2].hexbin(y, sp_inc_angle, mincnt=1, cmap='viridis')\n",
    "\n",
    "ax[0].set_xlabel('Wind speed (m/s)')\n",
    "ax[0].set_ylabel('DDM NBRCS')\n",
    "\n",
    "ax[1].set_xlabel('Wind speed (m/s)')\n",
    "ax[1].set_ylabel('DDM LES')\n",
    "\n",
    "ax[2].set_xlabel('Wind speed (m/s)')\n",
    "ax[2].set_ylabel('SP INC ANGLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836ab7a",
   "metadata": {},
   "source": [
    "## <font color=\"#ffcc00\">Feature engineering</font>\n",
    "\n",
    "We can use domain knowledge to compute new features from the existing variables. This is more commonly used for *classical machine learning algorithms* such as random forests. We will skip this step here and work only with the existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc97960",
   "metadata": {},
   "source": [
    "## <font color=\"#ff9900\">Model development</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f444d64",
   "metadata": {},
   "source": [
    "Predictive modeling includes setting up a machine learning algorithm, training it and evaluating its performance. Our algorithm of choice is a neural network. We will come to the neural network code in a minute, but first we need to prepare the data.\n",
    "\n",
    "### Prepare the input data\n",
    "\n",
    "The data is split into *train*, *validation*, and *test* data. All three datasets have their distinct purpose:\n",
    "1. Train data is given to the machine learning algorithm to tune the parameters of the neural network\n",
    "1. Validation data is used to identify when the machine learning algorithm starts to overfit to the training data (we want to avoid learning the training data by heart)\n",
    "1. Test data is used to gauge the ability of an ML algorithm to generalize. This dataset was not included at all in training and validation. We set it aside for now\n",
    "\n",
    "**Read the documentation string of the following function that we defined already for you. What does it provide?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(split, network='dense', normalize=True, verbose=True):\n",
    "    '''\n",
    "    Helper function to load the datasets that were prepared for this tutorial.\n",
    "    \n",
    "    Parameters:\n",
    "    split       - Choice of [train, valid, test]\n",
    "    network     - Choice of [dense, cnn]. Selects the appropriate 1D or 2D variables\n",
    "    normalize   - Normalize features (default: True)\n",
    "    verbose     - Print dataset information (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    (X, y) - Tuple of features and labels\n",
    "    '''   \n",
    "    \n",
    "    if network == 'dense':\n",
    "        input_keys = ['ddm_nbrcs', 'ddm_les', 'sp_inc_angle']\n",
    "    elif network == 'cnn':\n",
    "        input_keys = ['brcs']\n",
    "    else:\n",
    "        raise ValueError('Unknown option: network = ', network)\n",
    "    \n",
    "    ds = xr.open_dataset(f'../data/{split}_data.nc')\n",
    "    \n",
    "    X = []\n",
    "    \n",
    "    for key in input_keys:\n",
    "        var = ds[key][:]\n",
    "        if normalize:\n",
    "            var /= np.max(var)\n",
    "        X.append(var)\n",
    "        \n",
    "    X = np.swapaxes(np.asarray(X), 0, 1)\n",
    "    \n",
    "    if len(X.shape) == 4: # images to channel_last\n",
    "        X = np.swapaxes(X, 1, 3)\n",
    "    \n",
    "    y = np.array(ds['windspeed'][:])\n",
    "    y = y[:, np.newaxis]\n",
    "    \n",
    "    print(f'Loaded data for split {split}')\n",
    "    print(f'Feature array: {X.shape}')\n",
    "    print(f'Label array:   {y.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524c1ab-49a4-4c08-8cf2-686bdb7b0ad6",
   "metadata": {},
   "source": [
    "We use the function to create train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset('train')\n",
    "X_valid, y_valid = create_dataset('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ba65d",
   "metadata": {},
   "source": [
    "### Introduction to neural networks\n",
    "\n",
    "A single neuron takes an input $x$, applies a linear transformation $y = w \\cdot x + b$, and ultimately applies a non-linear *activation function* $\\sigma$, e.g., the relu function.\n",
    "\n",
    "Therefore, a single neuron transforms the input $x$ like:\n",
    "\n",
    "$y = \\sigma( w \\cdot x + b )$\n",
    "\n",
    "The parameters $w, b$ are *learned* by exposing the neural network to training data. The dense neural network is a neural network that stacks several individual neurons together in *layers*. A forward pass through such a network can be written as \n",
    "\n",
    "$y = \\sigma A( \\sigma B (x))$\n",
    "\n",
    "where $A, B$ are the weight matrices of the neural network.\n",
    "\n",
    "<img src=\"./images/dense-neural-network.png\" size=\"0.5\">\n",
    "\n",
    "### Define a neural network architecture\n",
    "\n",
    "For convenience, we define a python function that can generate dense neural networks of various sizes. We use the *Keras* machine learning framework (https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd033441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_nn(H0=16, H1=8, input_dim=(3,)):\n",
    "    '''\n",
    "    Create a dense neural network with two hidden layers\n",
    "    \n",
    "    Parameters:\n",
    "    H0 - Number of neurons in 1st hidden layer\n",
    "    H1 - Number of neurons in 2nd hidden layer\n",
    "    input_dim - Number of input features\n",
    "    '''\n",
    "    \n",
    "    # Create a Keras input tensor\n",
    "    inputs = keras.layers.Input(shape=input_dim)\n",
    "    \n",
    "    # Apply the first hidden layer\n",
    "    hidden_layer = keras.layers.Dense(H0, activation='relu')(inputs)\n",
    "    \n",
    "    # Apply the second hidden layer\n",
    "    hidden_layer2 = keras.layers.Dense(H1, activation='relu')(hidden_layer)\n",
    "    \n",
    "    # Reduce to one final output for the regression\n",
    "    outputs = keras.layers.Dense(1)(hidden_layer2) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c1c40",
   "metadata": {},
   "source": [
    "Create a model with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9be97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nn(H0=16, H1=8)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6c632",
   "metadata": {},
   "source": [
    "We also need to define an optimizer that is the strategy to reach a minimum of the neural network parameter space. In Keras, this is done by compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967654ad",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ebfb2",
   "metadata": {},
   "source": [
    "In training, we show the training data to the neural network such that it can estimate the parameters. Then, the loss is calculated, here the mean squared error of true ($y$) vs predicted ($\\hat y$) labels:\n",
    "\n",
    "$\\mathcal L = \\frac 1 N \\sum\\limits_{i=1}^N (y_i - \\hat y_i)^2$\n",
    "\n",
    "Based on that, the neural network weights are adapted using backward propagation (advanced topic). We show the data to the network in *minibatches* for scalability and efficiency. We call it an *epoch of training* when we showed every training sample once to the network. Thus, we iteratively approach a set of neural network parameters that corresponds to a local minimum of the loss function.\n",
    "\n",
    "An important question is how we should know that we should *stop* training. In theory, we could train forever and ultimately reduce the loss on the training set to 0. That would not be helpful, because the model would then not generalize well to unseen data, a phenomenon known as *overfitting*. Therefore, we monitor the loss on the validation set during training, and stop training once this loss does no longer decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=200 # stop here in any case\n",
    "batch_size=32  # size of the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f03b47",
   "metadata": {},
   "source": [
    "### Analyze the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059fcf3",
   "metadata": {},
   "source": [
    "Plot the history of the training process. The Keras framework automatically stored the training and validation loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_epochs = len(history.history['loss'])\n",
    "\n",
    "sns.lineplot(x=range(trained_epochs), y=history.history['loss'], label='Train loss')\n",
    "sns.lineplot(x=range(trained_epochs), y=history.history['val_loss'], label='Validation loss')\n",
    "\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "plt.xticks(range(trained_epochs))\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262a233",
   "metadata": {},
   "source": [
    "## <font color=\"#ff0000\">Verification</font>\n",
    "\n",
    "Improve the neural network\n",
    "\n",
    "### Single metric\n",
    "\n",
    "We need to define a strategy to gauge the performance of the neural network. For that, we recommend to choose a single metric that is determined on the validation set and that you optimize step by step. In our case, this is the root mean squared error (RMSE). Calculate it below for the model we trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) obtained on validation set: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df428915-66da-4cc4-8a87-b17d264a09d5",
   "metadata": {},
   "source": [
    "**Discuss: What would be the single metric in your machine learning project?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5b27c-1057-4d5a-ac33-6b7c22940fdb",
   "metadata": {},
   "source": [
    "## <font color=\"#ff3399\">Workflow tuning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4880b",
   "metadata": {},
   "source": [
    "### Next try\n",
    "\n",
    "Change the parameters `H0, H1` of the neural network, as well as the batch size. What do you observe for the validation set results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_nn(H0=128, H1=64)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "history = model2.fit(X_train, y_train, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_valid)\n",
    "\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) obtained on validation set: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667f659",
   "metadata": {},
   "source": [
    "Compare the RMSE to the RMSE you obtained before with the default architecture. Do you see an improvement?\n",
    "\n",
    "**Optional: Try out more values for H0, H1, and batch_size and record your resulting RMSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa23297",
   "metadata": {},
   "source": [
    "### Advanced topic: Convolutional neural network (CNN)\n",
    "\n",
    "Remember that the dataset contains 2D variables as well, which we did not use so far. These can be seen as images. Convolutional neural networks originated in computer vision and were originally developed for the image classification. We adapt here a convolutional neural network for regression.\n",
    "\n",
    "<img src=\"./images/d4g-cnn-sketch.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6017afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(n_filters=16, H0=64, H1=32):\n",
    "    '''\n",
    "    Create a convolutional neural network. The architecture has 2 convolutional layers, followed by two dense layers.\n",
    "    \n",
    "    Parameters:\n",
    "    n_filters - number of filters in the convolutional layer\n",
    "    H0        - number of neurons in the dense layer\n",
    "    '''\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(11, 17, 1))\n",
    "    conv_layer1 = keras.layers.Conv2D(n_filters, 3, activation=\"relu\")(inputs)\n",
    "    pooling_layer = keras.layers.MaxPool2D()(conv_layer1)\n",
    "    conv_layer2 = keras.layers.Conv2D(n_filters, 3, activation=\"relu\")(pooling_layer)\n",
    "    pooling_layer2 = keras.layers.MaxPool2D()(conv_layer2)\n",
    "    flatten_layer = keras.layers.Flatten()(pooling_layer2)\n",
    "    dense_layer = keras.layers.Dense(H0, activation=\"relu\")(flatten_layer)\n",
    "    dense_layer2 = keras.layers.Dense(H1, activation=\"relu\")(dense_layer)\n",
    "    outputs = keras.layers.Dense(1)(dense_layer2)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900307c8",
   "metadata": {},
   "source": [
    "We create training and validation data this time using the image data part of the provided CyGNSS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, _ = create_dataset('train', network='cnn')\n",
    "X_valid_cnn, _ = create_dataset('valid', network='cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e87edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = create_cnn(n_filters=16, H0=64, H1=32)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "history = model_cnn.fit(X_train_cnn, y_train, \n",
    "                    validation_data=(X_valid_cnn, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_valid, model_cnn.predict(X_valid_cnn), squared=False)\n",
    "print(f'RMSE for the CNN: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d556f8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "To summarize the results that were obtained on the *validation* set:\n",
    "\n",
    "**Fill the table with your results**\n",
    "\n",
    "1. Dense neural network\n",
    "\n",
    "| H0 | H1 | batch size | RMSE |\n",
    "|--  |--  |--          | --   |\n",
    "| 16 | 8  | 32        | ... m/s |\n",
    "| 128 | 64 | 32       | ... m/s |\n",
    "\n",
    "\n",
    "2. Convolutional neural network\n",
    "\n",
    "| Filters | H0 | H1 | batch size | RMSE |\n",
    "|--       |--  |--  |--          |--    |\n",
    "| 16      | 64 | 32 | 32         | ... m/s |\n",
    "| 32      | 128 | 64 | 32        | ... m/s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd292e56",
   "metadata": {},
   "source": [
    "## <font color=\"#990099\">Evaluation and roll-out</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4b386",
   "metadata": {},
   "source": [
    "Finally, we use our model to predict wind speeds for the test set. So far, we did not touch this dataset, so it should give an idea how well our machine learning algorithm generalizes to unseen data. We repeat the dataset preparation for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = create_dataset('test', network='dense')\n",
    "X_test_cnn, _ = create_dataset('test',  network='cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd39884",
   "metadata": {},
   "source": [
    "Choose one of the model architectures that you think perform well on the given dataset. If necessary, train this model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here: Train again if necessary\n",
    "best_model = model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41dbbd7-4899-4b35-a095-e3a92f96e042",
   "metadata": {},
   "source": [
    "Use the trained model to make predictions on the test set. Un-comment the line corresponding to your best architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = best_model.predict(X_test_cnn) # if CNN was best model\n",
    "y_pred = best_model.predict(X_test) # if ANN was best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8b566",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73fd85a",
   "metadata": {},
   "source": [
    "Calculate metrics to report on the performance of your machine learning algorithm. Compare the test set RMSE with the validation RMSE. \n",
    "\n",
    "**Discuss: What do you observe? Is your RMSE on the test larger or smaller than the RMSE obtained on the validation set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6be20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) for the test set: {rmse:.4} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7c9ae",
   "metadata": {},
   "source": [
    "### Histogram plot\n",
    "\n",
    "In a regression problem, it is interesting to see the performance of the machine learning algorithm beyond the aggregated RMSE metric. We plot the histogram of true windspeed and predicted windspeed. What do you observe? Can you identify a windspeed range where our machine learning algorithm performs poorly? What are possible explanations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "sns.distplot(y_test.squeeze(), color='gray', label='True wind speed', ax=ax)\n",
    "sns.distplot(y_pred.squeeze(), color='C2', label='Predicted wind speed', ax=ax)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Wind speed (m/s)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D scatter plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "img = ax.hexbin(y_test.squeeze(), y_pred.squeeze(), mincnt=1, cmap='viridis')\n",
    "plt.colorbar(img, label='Sample density')\n",
    "\n",
    "ax.set_xlabel('ERA5 wind speed (m/s)')\n",
    "ax.set_ylabel('Predicted wind speed (m/s)')\n",
    "\n",
    "xmin = 0\n",
    "xmax = 20\n",
    "\n",
    "ax.plot(np.linspace(xmin, xmax), np.linspace(xmin, xmax), 'r--')\n",
    "\n",
    "ax.set_ylim(xmin, xmax)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(xmin, xmax + 2.5, 2.5))\n",
    "ax.set_yticks(np.arange(xmin, xmax + 2.5, 2.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf906018",
   "metadata": {},
   "source": [
    "**Discuss: What do you think of the network's performance? What could be done to increase the performance? Can you identify samples that are harder to predict correctly than others?** \n",
    "\n",
    "*Related publication:*\n",
    "\n",
    "Asgarimehr, M., Arnold, C., Weigel, T., Ruf, C. & Wickert, J. GNSS reflectometry global ocean wind speed using deep learning: Development and assessment of CyGNSSnet. Remote Sensing of Environment 269, 112801 (2022).\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c05f6-bc86-4c18-b8b6-ec564ca4ba02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
