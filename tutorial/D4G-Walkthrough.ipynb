{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216df692",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "D4G workshop\n",
    "\n",
    "Potsdam, 13.06.22\n",
    "\n",
    "Author: Caroline Arnold, DKRZ / Helmholtz AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca8ade",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to Introduction to Machine Learning! Today we are going to work through the typical lifecycle of a machine learning project. We will be working with data from the CyGNSS satellite mission to predict global ocean wind speed.\n",
    "\n",
    "<img src=\"./images/data-science-lifecycle.png\" alt=\"Data Science Lifecycle\">\n",
    "\n",
    "### Setup\n",
    "\n",
    "This tutorial could be done on a laptop, but for convenience we will use Google Colab.\n",
    "\n",
    "Now sign into Google Colab and clone the git repository using the following commands\n",
    "\n",
    "```bash\n",
    "TODO clone command\n",
    "```\n",
    "\n",
    "The data is stored separately in DKRZ nextcloud. Download it by\n",
    "\n",
    "```bash\n",
    "TODO download address for the data\n",
    "TODO check how to get the data into Google Colab\n",
    "```\n",
    "\n",
    "### Today's Goals\n",
    "\n",
    "1. Walk through all stages of a machine learning project\n",
    "1. Train a neural network using the Keras framework\n",
    "1. Learn strategies to improve your machine learning algorithm\n",
    "1. Optional: Get familiar with different neural network architectures\n",
    "\n",
    "This notebook can serve as a reference for you to employ in your own scientific machine learning projects. We do not expect you to understand every single line of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08de13",
   "metadata": {},
   "source": [
    "## Understanding the science case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc719dd",
   "metadata": {},
   "source": [
    "The CyGNSS (Cyclone GNSS) is a system of microsatellites that measures GNSS reflected off the Earth's surface. We would like to predict the global ocean wind speed from CyGNSS measurements. This is a regression problem (the wind speed is a continuous variable).\n",
    "\n",
    "<img src=\"./images/cygnss-from-space.png\" alt=\"CyGNSS satellites and transmitter on top of a cyclone\" title=\"CyNGSS satellites, Image courtesy of Milad Asgarimehr\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe66e4a",
   "metadata": {},
   "source": [
    "Question for you: TODO drop this and add more pictures\n",
    "- What is the science case in your machine learning project?\n",
    "- What are you trying to predict?\n",
    "- Is it a classification or a regression problem or something else?\n",
    "\n",
    "Take some minutes to discuss in small groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0dde",
   "metadata": {},
   "source": [
    "## Data mining\n",
    "\n",
    "For machine learning, data is the most important ingredient. For the purpose of this tutorial, we already retrieved data (TODO add more context?) and prepared a subset of CyGNSS data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5f435",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "While it may be easy to obtain a lot of data, it is necessary to ensure the data quality, eg by checking for `None` values in the data. For the purpose of this tutorial, you can assume the data has been cleaned such that you can directly work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5b07f",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Now it is time to take a look at the data. We use the Python library `xarray` to open the `netcdf` files that are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472ee95-ab38-4941-b131-2c178c2f8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "np.random.seed(20220613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b5476-f986-46c0-b1ad-33dc90662b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change this to xarray later\n",
    "ds_train = h5py.File('../data/train_data.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f4f3a-6640-432b-a67c-49c242a29b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc547f7",
   "metadata": {},
   "source": [
    "### Target variable\n",
    "\n",
    "The target variable is the wind speed. To extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ds_train['windspeed'][:] # TODO change this to xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abd0cb",
   "metadata": {},
   "source": [
    "Visualization is very helpful for machine learning projects, as it helps us to identify the key properties of the dataset at a glance. We plot the distribution of the wind speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e346970-598c-47dd-b106-f90feadedb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries for plotting. Check out https://seaborn.pydata.org/ for reference\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1ccfa-dd0e-439e-ada1-4c0306f99385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y)\n",
    "\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.xticks([0, 2.5, 5.0, 7.5, 10, 12.5, 15, 17.5, 20])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2194f0e-5523-48c1-aa35-8e5bfcc3d7ba",
   "metadata": {},
   "source": [
    "### Feature variables\n",
    "\n",
    "We can use the interactive xarray dataset browser TODO check if that works in Google Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a151c5-f33d-4495-b447-9b9c7762265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bff76-47cf-47ba-a926-baf92227714e",
   "metadata": {},
   "source": [
    "There are 2D and 1D variables in the dataset. First, we extract the 2D variables and look at some selected samples.\n",
    "\n",
    "#### BRCS (Bistatic Radar Cross Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f8d03-e0bc-4e9a-8a23-d4f67a6f1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brcs = ds_train['brcs'][:]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "for i in range(5):\n",
    "    sns.heatmap(brcs[i*100], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbec7bf-3d5b-492a-aeb2-09a3e5716501",
   "metadata": {},
   "source": [
    "#### Effective scatter map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5aa93f-53e1-4b8f-ad2e-14777c1edbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_scatter = ds_train['eff_scatter'][:]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "for i in range(5):\n",
    "    sns.heatmap(eff_scatter[i*100], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0966c-f783-4715-aad5-6d56df443c38",
   "metadata": {},
   "source": [
    "The dataset contains 1D variables as well. Here we can see the value ranges using histogram plots\n",
    "\n",
    "#### Normalized bistatic radar cross section (ddm_nbrcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6468a7b-ad0a-4e4a-a2e7-4a5d106069cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_nbrcs = ds_train['ddm_nbrcs'][:]\n",
    "#ddm_les   = ds_train['ddm_les'][:]\n",
    "\n",
    "sns.histplot(ddm_nbrcs)\n",
    "\n",
    "plt.xlabel('DDM NBRCS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brcs = ds.brcs.values # TODO change this later to xarray\n",
    "\n",
    "plt.imshow(brcs[0])\n",
    "\n",
    "ddm_nbrcs = ds.ddm_nbrcs.values\n",
    "ddm_les   = ds.ddm_les.values\n",
    "# TODO add an additional variable that is not so promising?\n",
    "# TODO remove the quality variable\n",
    "\n",
    "plt.histogram(ddm_nbrcs)\n",
    "plt.histogram(ddm_les)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30846194",
   "metadata": {},
   "source": [
    "### Relation of feature and target variables\n",
    "\n",
    "TODO introduce the math when the neural network is introduced\n",
    "\n",
    "A machine learning algorithm can learn to approximate a function\n",
    "\n",
    "$y = f(X)$\n",
    "\n",
    "where it learns the function $f$ based on data. Compared to traditional fitting algorithms, we do  not need to specify $f$ explicitly. A neural network can be trained to replace any kind of \"well-behaved\" non-linear function.\n",
    "\n",
    "Let's take a look at the 2D density plots of the features and target variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, figsize=(20, 6))\n",
    "\n",
    "ax[0]. # ddm_nbrcs\n",
    "ax[1]. # ddm_les\n",
    "ax[2]. # the third variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188dd22-4ea5-4b0b-a39f-96d821906ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO temp\n",
    "\n",
    "plt.hexbin(y, ddm_nbrcs, mincnt=1, cmap='viridis')\n",
    "\n",
    "plt.xlabel('Wind speed (m/s)')\n",
    "plt.ylabel('DDM NBRCS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6886a1",
   "metadata": {},
   "source": [
    "*Question* TODO maybe too complicated\n",
    "- Based on these plots, which variables would you select as input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45c672",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b1b06",
   "metadata": {},
   "source": [
    "This step was more important for classical machine learning algorithms, where data scientists provide handcrafted additional features to the machine learning algorithm. We skip this step here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f425fd",
   "metadata": {},
   "source": [
    "## Predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73111a",
   "metadata": {},
   "source": [
    "Predictive modeling includes setting up a machine learning algorithm, training it and evaluating its performance. Our algorithm of choice is a neural network. We have prepared all the necessary code for you to train the algorithm.\n",
    "\n",
    "### Prepare the input data\n",
    "\n",
    "The data is split into *train*, *validation*, and *test* data. All three datasets have their distinct purpose:\n",
    "1. Train data is given to the machine learning algorithm to tune the parameters of the neural network\n",
    "1. Validation data is used to identify when the machine learning algorithm starts to overfit to the training data (we want to avoid learning the training data by heart)\n",
    "1. Test data is used to gauge the ability of an ML algorithm to generalize. This dataset was not included at all in training and validation. We set it aside for now\n",
    "\n",
    "TODO add image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb141642-2bce-45f7-9607-6a88d57e8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(split, input_keys=['ddm_nbrcs'], normalize=True, verbose=True):\n",
    "    '''\n",
    "    Helper function to load the datasets that were prepared for this tutorial.\n",
    "    \n",
    "    Parameters:\n",
    "    split       - Choice of [train, valid, test]\n",
    "    input_keys  - Input parameters (need to be all 1D or all 2D)\n",
    "    normalize   - Normalize features (default: True)\n",
    "    verbose     - Print dataset information (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    (X, y) - Tuple of features and labels\n",
    "    '''\n",
    "    # TODO change this to xarray\n",
    "    \n",
    "    ds = h5py.File(f'../data/{split}_data.h5', 'r')\n",
    "    \n",
    "    X = []\n",
    "    \n",
    "    for key in input_keys:\n",
    "        var = ds[key][:]\n",
    "        if normalize:\n",
    "            var /= np.max(var)\n",
    "        X.append(var)\n",
    "        \n",
    "    X = np.swapaxes(np.asarray(X), 0, 1)\n",
    "    \n",
    "    if len(X.shape) == 4: # images to channel_last\n",
    "        X = np.swapaxes(X, 1, 3)\n",
    "    \n",
    "    y = ds['windspeed'][:]\n",
    "    y = y[:, np.newaxis]\n",
    "    \n",
    "    print(f'Loaded data for split {split}')\n",
    "    print(f'Feature array: {X.shape}')\n",
    "    print(f'Label array:   {y.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset('train', input_keys=['ddm_nbrcs', 'ddm_nbrcs'])\n",
    "X_valid, y_valid = create_dataset('valid', input_keys=['ddm_nbrcs', 'ddm_nbrcs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74849e9-a5d5-4c8e-907c-b1fcdb6ced35",
   "metadata": {},
   "source": [
    "### Introduction to neural networks\n",
    "\n",
    "A single neuron takes an input $x$, applies a linear transformation $y = w \\cdot x + b$, and ultimately applies a non-linear *activation function* $\\sigma$, e.g., the relu function.\n",
    "\n",
    "Therefore, a single neuron transforms the input like:\n",
    "\n",
    "$y = \\sigma( w \\cdot x + b )$\n",
    "\n",
    "The parameters $w, b$ are *learned* by exposing the neural network to training data. The dense neural network is a neural network that stacks several individual neurons together in *layers*. A forward pass through such a network can be written as \n",
    "\n",
    "$y = \\sigma A( \\sigma B (x))$\n",
    "\n",
    "where $A, B$ are the weight matrices of the neural network.\n",
    "\n",
    "<img src=\"./images/dense-neural-network.png\" size=\"0.5\">\n",
    "\n",
    "### Define a neural network architecture\n",
    "\n",
    "For convenience, we define a python function that can generate dense neural networks with various sizes. We use the *Keras* machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59051ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_nn(H0=16, H1=8, input_dim=(2,)):\n",
    "    '''\n",
    "    Create a dense neural network with two hidden layers\n",
    "    \n",
    "    Parameters:\n",
    "    H0 - Number of neurons in 1st hidden layer\n",
    "    H1 - Number of neurons in 2nd hidden layer\n",
    "    input_dim - Number of input features\n",
    "    '''\n",
    "    \n",
    "    # Create a Keras input tensor\n",
    "    inputs = keras.layers.Input(shape=input_dim)\n",
    "    \n",
    "    # Apply the first hidden layer\n",
    "    hidden_layer = keras.layers.Dense(H0, activation='relu')(inputs)\n",
    "    \n",
    "    # Apply the second hidden layer\n",
    "    hidden_layer2 = keras.layers.Dense(H1, activation='relu')(hidden_layer)\n",
    "    \n",
    "    # Reduce to one final output for the regression\n",
    "    outputs = keras.layers.Dense(1)(hidden_layer2) \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e818e42",
   "metadata": {},
   "source": [
    "Create a model with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93331a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nn(H0=16, H1=8)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811e5aa",
   "metadata": {},
   "source": [
    "We also need to define an optimizer that is the strategy to reach a minimum of the neural network parameter space. In Keras, this is done by compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d718f0-7189-4309-a18c-29e4ef8b1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54719d24",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5856c6c",
   "metadata": {},
   "source": [
    "In training, we show the training data to the neural network such that it can estimate the parameters. Then, the loss is calculated, here the mean squared error of true ($y$) vs predicted ($\\hat y$) labels:\n",
    "\n",
    "$\\mathcal L = \\frac 1 N \\sum\\limits_{i=1}^N (y_i - \\hat y_i)^2$\n",
    "\n",
    "Based on that, the neural network weights are adapted using backward propagation (advanced topic). We show the data to the network in *minibatches* for scalability and efficiency.\n",
    "\n",
    "An important question is how we should know that we should *stop* training. In theory, we could train forever and ultimately reduce the loss on the training set to 0. That would not be helpful, because the model would then not generalize well to unseen data, a phenomenon known as *overfitting*. Therefore, we monitor the loss on the validation set during training, and stop training once this loss does no longer decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302c85d-4808-435d-940b-6abea9f01f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=200 # stop here in any case\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e126f-f59d-4092-96c7-c959d090bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8807cd-0011-48c2-9802-05220a486c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f769d",
   "metadata": {},
   "source": [
    "### Analyze the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fd6c1",
   "metadata": {},
   "source": [
    "Plot the history of the training process. The Keras framework automatically stored the training and validation loss for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d421a-f3f2-4b27-a743-eeb09367935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_epochs = len(history.history['loss'])\n",
    "\n",
    "sns.lineplot(x=range(trained_epochs), y=history.history['loss'], label='Train loss')\n",
    "sns.lineplot(x=range(trained_epochs), y=history.history['val_loss'], label='Validation loss')\n",
    "\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "plt.xticks(range(trained_epochs))\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071a322-5632-4d84-a7d4-ace8341cf6dc",
   "metadata": {},
   "source": [
    "### Improve the neural network\n",
    "\n",
    "#### Single metric\n",
    "\n",
    "We need to define a strategy to gauge the performance of the neural network. For that, we recommend to choose a single metric that is determined on the validation set and that you optimize step by step. In our case, this is the root mean squared error (RMSE). Calculate it below for the model we trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d26de-37bf-4261-a773-72a0fb3c849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) obtained on validation set: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c3861-7c66-448b-9836-d3de63af332a",
   "metadata": {},
   "source": [
    "#### Next try\n",
    "\n",
    "Change the parameters `H0, H1` of the neural network, as well as the batch size. What do you observe for the validation set results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396c44d-8d4d-4335-ad79-c4f786498f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_nn(H0=128, H1=64)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfeaa2-6bb3-4d98-886d-d8814dd25c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "history = model2.fit(X_train, y_train, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191734b3-c1ab-4ba2-9de3-bd8c4de5a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_valid)\n",
    "\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) obtained on validation set: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da3e88-513a-400f-bbca-ea27ffc7259f",
   "metadata": {},
   "source": [
    "Compare the RMSE to the RMSE you obtained before with the default architecture. Do you see an improvement?\n",
    "\n",
    "Optional: Try out more architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233bb1f",
   "metadata": {},
   "source": [
    "### Advanced topic: Convolutional neural network (CNN)\n",
    "\n",
    "Remember that the dataset contains 2D variables as well, which we did not use so far. Convolutional neural networks originated in computer vision and were originally developed for the image classification. We adapt here a convolutional neural network for regression.\n",
    "\n",
    "TODO add sketch of network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(n_filters=16, H0=64, H1=32):\n",
    "    '''\n",
    "    Create a convolutional neural network. The architecture has 2 convolutional layers, followed by two dense layers.\n",
    "    \n",
    "    Parameters:\n",
    "    n_filters - number of filters in the convolutional layer\n",
    "    H0        - number of neurons in the dense layer\n",
    "    '''\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(11, 17, 2))\n",
    "    conv_layer1 = keras.layers.Conv2D(n_filters, 3, activation=\"relu\")(inputs)\n",
    "    conv_layer2 = keras.layers.Conv2D(n_filters, 3, activation=\"relu\")(conv_layer1)\n",
    "    pooling_layer = keras.layers.MaxPool2D()(conv_layer2)\n",
    "    flatten_layer = keras.layers.Flatten()(pooling_layer)\n",
    "    dense_layer = keras.layers.Dense(H0, activation=\"relu\")(flatten_layer)\n",
    "    dense_layer2 = keras.layers.Dense(H1, activation=\"relu\")(dense_layer)\n",
    "    outputs = keras.layers.Dense(1)(dense_layer2)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9452931",
   "metadata": {},
   "source": [
    "We create training and validation data this time using the image data part of the provided CyGNSS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e3da8-d0bd-40ea-a266-4101ca471c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, _ = create_dataset('train', input_keys=['brcs', 'eff_scatter'])\n",
    "X_valid_cnn, _ = create_dataset('valid', input_keys=['brcs', 'eff_scatter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51185-5bb9-4515-aeed-18697cb2d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = create_cnn()\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907904a-79c5-4ed6-be06-c4dfb56b5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "history = model_cnn.fit(X_train_cnn, y_train, \n",
    "                    validation_data=(X_valid_cnn, y_valid), \n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=max_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af44d1-8414-4fda-98f4-3c46673f56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_valid, model_cnn.predict(X_valid_cnn), squared=False)\n",
    "print(f'RMSE for the CNN: {rmse:.4f} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79294467-6541-499d-9784-4fcfec034209",
   "metadata": {
    "tags": []
   },
   "source": [
    "To summarize the results that were obtained on the *validation* set:\n",
    "\n",
    "1. Dense neural network\n",
    "\n",
    "| H0 | H1 | batch size | RMSE |\n",
    "|--  |--  |--          | --   |\n",
    "| 16 | 8  | 32        | TODO |\n",
    "\n",
    "\n",
    "2. Convolutional neural network\n",
    "\n",
    "| Filters | H0 | H1 | batch size | RMSE |\n",
    "|--       |--  |--  |--          |--    |\n",
    "| 16      | 32 | 16 | 32         | TODO |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174c449",
   "metadata": {},
   "source": [
    "### Final step: Test set prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c09115-9fe0-4aec-8121-86d163764394",
   "metadata": {},
   "source": [
    "Repeat the dataset preparation for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef64af6-eff6-4992-a379-83f42dc37ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = create_dataset('test', input_keys=['ddm_nbrcs', 'ddm_nbrcs'])\n",
    "X_test_cnn, _ = create_dataset('test', input_keys=['brcs', 'eff_scatter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e02618",
   "metadata": {},
   "source": [
    "Choose one of the model architectures that you think perform well on the given dataset. If necessary, train this model again. Use the trained model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO train again if necessary\n",
    "\n",
    "best_model = model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df132eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_cnn) # if CNN was best model\n",
    "# y_pred = best_model.predict(X_test) # if ANN was best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb942014",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f54398",
   "metadata": {},
   "source": [
    "Calculate metrics to report on the performance of your machine learning algorithm. Compare the test set RMSE with the validation RMSE. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'Root mean squared error (RMSE) for the test set: {rmse:.4} m/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac697f27-3fe3-4db9-bed8-3eae819f4c7f",
   "metadata": {},
   "source": [
    "### Histogram plot\n",
    "\n",
    "In a regression problem, it is interesting to see the performance of the machine learning algorithm beyond the aggregated RMSE metric. We plot the histogram of true windspeed and predicted windspeed. What do you observe? Can you identify a windspeed range where our machine learning algorithm performs poorly? What are possible explanations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721f275-4a87-4037-9b77-dc61a6f51483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "sns.distplot(y_test.squeeze(), color='gray', label='True wind speed', ax=ax)\n",
    "sns.distplot(y_pred.squeeze(), color='C2', label='Predicted wind speed', ax=ax)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Wind speed (m/s)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4564aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D scatter plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "img = ax.hexbin(y_test.squeeze(), y_pred.squeeze(), mincnt=1, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('ERA5 wind speed (m/s)')\n",
    "ax.set_ylabel('Predicted wind speed (m/s)')\n",
    "\n",
    "xmin = 0\n",
    "xmax = 20\n",
    "\n",
    "ax.plot(np.linspace(xmin, xmax), np.linspace(xmin, xmax), 'r--')\n",
    "\n",
    "ax.set_ylim(xmin, xmax)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "\n",
    "plt.colorbar(img, label='Sample density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0944e-bb94-4ce0-bddc-0bb0ee4b9284",
   "metadata": {},
   "source": [
    "Discussion: Overall, it is possible to obtain reasonable windspeed predictions even with the small set of training samples that was provided here in the tutorial. However, the prediction tends towards the mean windspeed value, which means that low windspeeds are overestimated and high windspeeds are underestimated. This is known as *regression to the mean* and applies to all statistical algorithms. Note that the windspeed distribution is not uniform, and therefore the algorithm is presented more often with average samples compared to samples with high windspeed.\n",
    "\n",
    "Related publication:\n",
    "\n",
    "Asgarimehr, M., Arnold, C., Weigel, T., Ruf, C. & Wickert, J. GNSS reflectometry global ocean wind speed using deep learning: Development and assessment of CyGNSSnet. Remote Sensing of Environment 269, 112801 (2022).\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f31cf7-ecb2-4f1f-a639-012ef252b6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
